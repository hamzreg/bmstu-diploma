\chapter{Анализ предметной области}

\section{Основные определения}

Под информацией понимают сведения, которые являются объектом хранения, передачи и обработки \cite{information}. Формой представления информации является сообщение. Физическую величину, отображающую сообщение, называют сигналом. Передача информации осуществляется следующим образом \cite{transmission}:

\begin{enumerate}
	\item Источник информации создает случайное сообщение. В теории информации любой источник информации является стохастическим, его можно описать измеряемыми вероятностными категориям.
	\item Сообщение поступает в систему передачи, в которой выполняется кодирование --- преобразование сообщения c целью согласования источника информации с каналом связи для увеличения скорости передачи информации или обеспечения заданной помехоустойчивости  \cite{information}. Кодирование состоит из шифрования, сжатия и защиты от шума, в результате которых формируется сигнал. 
	\item Сигнал проходит через канал --- среду передачи информации \cite{transmission}. В канале могут возникать помехи, создаваемые источником шума.
	\item Сигнал подается на вход системе приема, которая выполняет декодирование --- восстановление исходного сообщения \cite{information}.
	\item Исходное сообщение передается получателю.
\end{enumerate}

Описанная схема передачи информации показана на рисунке \ref{img:transmission}.

\includeimage
    {transmission}
    {f}
    {h}
    {1\textwidth}
    {Схема передачи информации}
    
Сообщение содержит сведения о некоторой физической системе $X$, которая случайным образом может перейти в какое-либо состояние $x_{i}$ из конечного множества состояний $x_{1}, x_{2}, \dots, x_{n}$ c вероятностями $p_{1}, p_{2}, \dots, p_{n}$, где $n \in \mathbb{N}$, $p_{i} = P(X \sim x_{i})$ и $\sum_{i = 1}^n p_{i} = 1$. То есть, для такой системы существует степень неопределенности, которая описывается числом ее возможных состояний и их вероятностями. Сведения из принятого сообщения тем ценнее, чем больше была неопределенность системы до получения сообщения. Специальную характеристику, используемую в качестве меры неопределенности системы, называют информационной энтропией \cite{definition}. Информационная энтропия конечной вероятностной схемы определяется по формуле Шеннона:

\begin{equation}\label{entropy}
	H(X) = -\sum_{i = 1}^n (p_{i} \cdot \log_{a}p_{i}),
\end{equation}

\noindentгде $p_{i} \in [0, 1]$, $a > 1$.

Основание логарифма определяет единицы измерения информационной энтропии: при $a = 2$ энтропия измеряется в битах, при $a = 3$ --- в тритах, при $a = e$ --- в натах.

\section{Свойства информационной энтропии}\label{properties}

Информационная энтропия обладает следующими свойствами \cite{properties}:

\begin{enumerate}
	\item Энтропия всегда неотрицательна. Значения $\log_{a} p_{i}$ в формуле (\ref{entropy}) принимают неположительные значения, так как $p_{i} \in [0, 1]$. Поэтому
	
\begin{equation}
	H(X) = -\sum_{i = 1}^n (p_{i} \cdot \log_{a} p_{i}) \geqslant 0.
\end{equation}

	\item\label{property2} Энтропия равна нулю, если состояние системы в точности известно заранее. Если известно состояние $x_{k}$, в которое перейдет система $X$, то вероятность этого состояния $p_{k}$ равна единице, вероятности других состояний равны нулю. Тогда
	
\begin{equation}
	p_{k} \cdot \log_{a} p_{k} = 1 \cdot \log_{a} 1 = 1 \cdot 0 = 0.
\end{equation}

В связи с тем, что $\lim\limits_{p \to 0} (p \cdot \log_{a}p) = 0$, другие слагаемые суммы в формуле~(\ref{entropy}) равны нулю. В этом случае 

\begin{equation}
	H(X) = -\sum_{i = 1}^n (p_{i} \cdot \log_{a} p_{i}) = 0.
\end{equation}

	\item\label{property3} Энтропия принимает наибольшее значение при условии, что все состояния равновероятны, то есть, $p_{1} = p_{2} = \dots = p_{n} = \frac{1}{n}$. Тогда
	
\begin{equation}
	H(X) = -\sum_{i = 1}^n (p_{i} \cdot \log_{a} p_{i}) = -\sum_{i = 1}^n (\frac{1}{n} \cdot \log_{a} \frac{1}{n}) = -\log_{a} \frac{1}{n} = \log_{a} n.
\end{equation}
	
\end{enumerate}

\section{Сжатие данных}

Неслучайные данные имеют некоторую структуру. Наличие у данных структуры, которую можно использовать для уменьшения их размера путем достижения такого представления данных, в котором никакая структура не выделяется, называют избыточностью. Сжатие данных --- это процесс преобразования исходных данных в их компактную форму путем распознавания и использования избыточности данных \cite{compression-definition}. Процесс сжатия состоит из двух этапов:

\begin{enumerate}
	\item Этап моделирования, который включает в себя распознавание избыточности для построения модели. Модель представляет собой набор данных и правил, используемых для обработки входных символов.
	\item Этап кодирования данных с использованием модели.
\end{enumerate}

Описанные этапы сжатия данных представлены на рисунке \ref{img:compression-steps}.

\includeimage
    {compression-steps}
    {f}
    {h}
    {0.6\textwidth}
    {Этапы сжатия данных}

В результате сжатия исходных данных $X$ получается их представление~$X_{\text{сж}}$. При восстановлении сжатые данные $X_{\text{сж}}$ преобразуются в представление~$Y$. На основании требований к восстановлению выделяют \cite{compression-classes}:

\begin{itemize}
	\item сжатие данных без потерь, при котором $Y = X$;
	\item сжатие данных с потерями, при котором $Y \not= X$.
\end{itemize}

Схемы сжатия данных без потерь и с потерями показаны на рисунке \ref{img:compression-classes}.

\includeimage
    {compression-classes}
    {f}
    {h}
    {0.8\textwidth}
    {Схемы сжатия данных без потерь и с потерями}

Для измерения производительности сжатия используют характеристику, которую называют коэффициентом сжатия и определяют следующим образом~\cite{compression-coefficient}:

\begin{equation}
	K_{\text{сж}} = \frac{L_{\text{исх}}}{L_{\text{сж}}},
\end{equation}

\noindentгде $L_{\text{исх}}$ --- объем исходных данных, $L_{\text{сж}}$ --- объем сжатых данных.

Данные, обладающие предсказуемой структурой, сокращают неопределенность системы меньше, чем сведения, в которых никакая структура не выделяется. Так как информационная энтропия является мерой неопределенности системы, то данные с выделяемой структурой, имеют низкое значение энтропии. Сведения, в которых закономерности не определяются, имеют высокое значение энтропии \cite{relation}. Так, чем меньше избыточность данных, тем выше значение их энтропии. То есть, информационная энтропия сжатых данных выше, чем ее значение до сжатия.

Согласно теореме Шеннона об источнике шифрования сигнал, обладающий размером $S$ и информационной энтропией $H$, не может быть сжат менее, чем до $S \cdot H$ битов без потери точности информации. Таким образом, на основании информационной энтропии исходных данных определяется теоретическая граница коэффициента сжатия \cite{theorem}.

В связи с применением операций сложения, умножения и логарифмирования при вычислении энтропии по формуле (\ref{entropy}), число которых растет с увеличением объема данных, встает задача выбора метода подсчета. Использование метода влияет на скорость и время определения информационной энтропии.