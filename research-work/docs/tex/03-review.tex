\chapter{Описание существующих методов подсчета}

\section{Метод скользящего окна}

При решении задач, связанных со сжатием, данные представляют собой массив байтов $X$ размером $N$. Байт состоит из восьми битов, каждый из которых кодирует одно из значений множества $\{0, 1\}$. Поэтому один байт может принимать значение из интервала $[0; 255]$ в десятичной системе счисления. 

В методе скользящего окна \cite{sliding-window-method} под окном понимают \textbf{сказать про подпоследовательность, а потом сказать, что метод будет описываться для 8 бит} рассматриваемый на текущем этапе байт данных, то есть, последовательность из восьми битов. В связи с тем, что на каждом этапе окно смещается на следующий байт, как показано на рисунке \ref{img:sliding-window}, оно называется скользящим.

\includeimage
    {sliding-window}
    {f}
    {h}
    {1\textwidth}
    {Проход по массиву байтов методом скользящего окна}
    
Вычисление информационной энтропии методом скользящего окна включает в себя два шага:

\begin{enumerate}
	\item Для каждого байта подсчитывается число его вхождений $n_{i}$ в массив байтов, где $i = \overline{0, N - 1}$.
	\item С учетом того, что вероятность появления байта в массиве
	
	\begin{equation}
		p_{i} = \frac{n_{i}}{N},
	\end{equation}
	
	информационная энтропия вычисляется по следующей формуле:
	
	\begin{equation}
		H(X) = -\sum_{i = 0}^{255} (p_{i} \cdot log_{2}p_{i}).
	\end{equation}
\end{enumerate}

Информационная энтропия, подсчитанная методом скользящего окна, принимает значения из интервала $[0;8]$ бит. При этом информационная энтропия принимает нулевое значение в случае, когда массив данных состоит из одинаковых байтов, и максимальное значение, равное восьми, если все байты в массиве различны (свойство \ref{property3}).

% Недостатком метода является то, что он рассчитывается только на том факте, что вероятности P появления 1 в сообщениях не зависят от вероятностей появления 0 и 1 в битах предыдущего сообщения. Если есть такой зависимость, расчетное значение энтропии Бернулли H A( ) будет завышена, а значение соответствующей ей избыточной информации – занижено. Однако такая ошибка в значении энтропии и избыточность информация вполне приемлема во многих случаях. Поэтому небольшое превышение расчетного значения энтропии Бернулли *H A( ) над его реальным значением не сильно исказит конечный результат, но время расчета энтропии можно значительно сократить. Другим недостатком метода является то, что биномиальные коэффициенты k Cn при больших длинах сообщений n могут достигать значений, которые трудно вычислить. Однако время расчета энтропии *H A( ) при их использовании все же намного меньше времени сложения статистических вероятностей j p сообщений по формуле (1). Кроме того, существует множество методов быстрого расчета биномиальных коэффициентов, ускоряющих их вычисление.

\section{Биномиальный метод}

В биномиальном методе вычисления информационной энтропии рассматривается бернуллиевский источник информации
В биномиальном методе вычисления информационной энтропии \cite{binomial-method} данные $X$ делятся на $N$ подпоследовательностей битов длины $n$. \textbf{Дать определение бернуллиевского источника информации.} Одна подпоследовательность рассматривается как сообщение, создаваемое бернуллиевским источником информации и состоящее из символов алфавита $B = \{0, 1\}$. Вероятность появления нуля и единицы в сообщении равны $p$ и $1 - p$ соответственно.

Вероятность того, что $i$-ое сообщение содержит $k$ единиц, где $k = \overline{0, n}$, вычисляется следующим образом:

\begin{equation}\label{pk}
	P_{k} = p^k \cdot (1 - p)^{(n - k)}.
\end{equation} 

Количество возможных сообщений, содержащих $k$ единиц, определяется как биномиальный коэффициент:

\begin{equation}\label{binomial-coefficient}
	C_{n}^k = \frac{n!}{k! \cdot (n - k)!}.
\end{equation}

В связи с тем, что $k$ может принимать значения из интервала $[0; n]$, число биномиальных коэффициентов, подсчитанных по формуле \ref{binomial-coefficient}, равно $n + 1$. По свойству суммы биномиальных коэффициентов число всех возможных сообщений

\begin{equation}
	N = \sum_{k = 0}^n C_{n}^k = 2^n.
\end{equation}

Это означает, что $N$ сообщений можно разбить на $n + 1$ классов эквивалентности. Тогда информационная энтропия бернуллиевского источника информации равна:

\begin{equation}\label{binomial-entropy}
	H(X) = -\sum_{k = 0}^n (C_{n}^k \cdot P_{k} \cdot log_{2}P_{k}).
\end{equation}

В соответствии с формулой \ref{binomial-entropy} биномиальный метод подсчета информационной энтропии состоит из следующих этапов:

\begin{enumerate}
	\item Исходные сообщения разбиваются на $n + 1$ классов эквивалентности, сообщения которых содержат $k = \overline{0, n}$ единиц.
	\item Для каждого класса $k$ рассчитывается биномиальный коэффициент~$C_{n}^k$.
	\item Определяется вероятность $p_{i}$ появления единицы в каждом $i$-ом сообщении.
	\item Вычисляются вероятности $P_{k}$ по формуле \ref{pk}.
	\item Суммируются произведения биномиальных коэффициентов $C_{n}^k$ и логарифмов вероятностей $P_{k}$ для всех $n + 1$ значений~$k$.
\end{enumerate}

\textbf{Привести пример}

В биномиальном методе определяются вероятности появления не подпоследовательности, а единицы в каждой подпоследовательности. Кроме того, расчет ускоряется за счет разделения исходной последовательности на классы эквивалентности, что приводит к сокращению операций сложения.

Однако в биномиальном методе расчета энтропии предполагается, что вероятность появления единицы в подпоследовательности не зависит от вероятностей появления нуля или единицы в битах предыдущей подпоследовательности. При наличии такой зависимости вычисленное значение энтропии будет завышено. Недостатком данного метода является трудоемкость вычисления биномиальных коэффициентов с увеличением длины подпоследовательности.
