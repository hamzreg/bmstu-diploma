\chapter{Описание существующих методов подсчета}

\section{Метод скользящего окна}

При решении задач, связанных со сжатием, данные $X$ представляют собой массив байтов размером $N$ \cite{bytes}. Байт состоит из восьми битов, каждый из которых кодирует одно из значений множества $\{0, 1\}$. Поэтому один байт может принимать значения из интервала от 0 до 255 включительно в десятичной системе счисления. 

В методе скользящего окна под окном понимают рассматриваемую на текущем этапе подпоследовательность данных размером $n$ \cite{sliding-window-method}. При подсчете энтропии данным методом необходима дополнительная память размером $2^n$ для хранения числа вхождений подпоследовательностей данных. Так как с увеличением размера окна, растет объем дополнительной памяти, в качестве окна выбирается минимально адресуемая единица памяти, которой является байт \cite{memory-unit}. В связи с тем, что на каждом этапе окно смещается на следующие восемь битов, как показано на рисунке \ref{img:sliding-window}, оно называется скользящим.

\includeimage
    {sliding-window}
    {f}
    {h}
    {1\textwidth}
    {Проход по массиву байтов методом скользящего окна}
    
Вычисление информационной энтропии методом скользящего окна включает в себя два шага:

\begin{enumerate}
	\item Для каждого возможного значения байта подсчитывается число его вхождений $n_{i}$ в массив байтов, где $i = \overline{0, 255}$.
	\item С учетом того, что вероятность появления байта в массиве $p_{i} = \frac{n_{i}}{N}$, информационная энтропия вычисляется по следующей формуле:
	
	\begin{equation}
		H(X) = -\sum_{i = 0}^{255} (p_{i} \cdot \log_{2}p_{i}).
	\end{equation}
\end{enumerate}

Так как первый шаг метода предполагает проход по массиву байтов размером $N$, а второй шаг --- проход по массиву числа вхождений подпоследовательностей данных размером $2^n$, временная сложность метода скользящего окна --- $O(N + 2^n)$. 

Информационная энтропия, подсчитанная методом скользящего окна, принимает значения из интервала $[0, 8]$ битов. При этом согласно свойству~\ref{property2} из раздела \ref{properties} информационная энтропия принимает нулевое значение в случае, когда массив данных состоит из одинаковых байтов, и в соответствии со свойством~\ref{property3} из раздела \ref{properties} максимальное значение, равное восьми, если все байты в массиве различны.

\section{Биномиальный метод}

В биномиальном методе вычисления информационной энтропии \cite{binomial-method} данные~$X$ рассматриваются как последовательность сообщений, генерируемых бернуллиевским источником --- источником информации, порождающим символы из алфавита $\{0; 1\}$ с вероятностями $1 - p$ и $p$ соответственно, причем $p \in (0, 1)$ и может быть неизвестно \cite{bernullie-source}. То есть, сообщение представляет собой последовательность битов длины $n$.

Вероятность того, что сообщение содержит $k$ единиц, где $k = \overline{0, n}$, вычисляется следующим образом:

\begin{equation}\label{pk}
	P_{k} = p^k \cdot (1 - p)^{(n - k)}.
\end{equation} 

Количество возможных сообщений, содержащих $k$ единиц, определяется как биномиальный коэффициент:

\begin{equation}\label{binomial-coefficient}
	C_{n}^k = \frac{n!}{k! \cdot (n - k)!}.
\end{equation}

В связи с тем, что $k$ может принимать значения из интервала $[0, n]$, число биномиальных коэффициентов, подсчитанных по формуле (\ref{binomial-coefficient}), равно $n + 1$. Это означает, что сообщения можно разбить на $n + 1$ классов эквивалентности. Тогда информационная энтропия вычисляется так:

\begin{equation}\label{binomial-entropy}
	H(X) = -\sum_{k = 0}^n (C_{n}^k \cdot P_{k} \cdot \log_{2}P_{k}).
\end{equation}

В соответствии с формулой (\ref{binomial-entropy}) биномиальный метод подсчета информационной энтропии состоит из следующих этапов:

\begin{enumerate}
	\item Исходные сообщения разбиваются на $n + 1$ классов эквивалентности, сообщения которых содержат $k = \overline{0, n}$ единиц.
	\item Для каждого класса эквивалентности рассчитывается биномиальный коэффициент~$C_{n}^k$.
	\item Определяются вероятности $p$ появления единицы в сообщениях.
	\item Вычисляются вероятности $P_{k}$ по формуле (\ref{pk}).
	\item Суммируются произведения биномиальных коэффициентов $C_{n}^k$ и логарифмов вероятностей $P_{k}$ для всех $n + 1$ значений~$k$.
\end{enumerate}

Для определения вероятности $p$ появления единицы в сообщениях на третьем этапе биномиального метода необходима дополнительная память размером $n + 1$. При этом данный этап подразумевает проход по массиву байтов размером $N$. Последний этап вычисления предполагает проход по массиву, содержащему значения вероятностей $p$ появления единицы в сообщениях. Тогда временная сложность биномиального метода подсчета информационной энтропии --- $O(N + n)$.

В биномиальном методе определяются вероятности появления не подпоследовательности, а единицы в подпоследовательностях. Кроме того, расчет ускоряется за счет разделения исходной последовательности на классы эквивалентности, что приводит к сокращению операций сложения.

При рассмотрении бернуллиевского источника информации предполагается, что вероятность появления единицы в подпоследовательности не зависит от вероятностей появления нуля или единицы в битах предыдущей подпоследовательности. При наличии такой зависимости вычисленное значение энтропии будет завышено. Так как рассматриваемое представление данных --- последовательность битов, то вероятности появления каждого значения бита независимы. 

Недостатком данного метода является трудоемкость вычисления биномиальных коэффициентов с увеличением длины подпоследовательности.
